{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Function to remove isolated letters followed by a period and space\n",
    "def clean_name(name):\n",
    "    return re.sub(r'\\b[A-Z]\\.\\s+', '', name)\n",
    "\n",
    "# Function to get Wikipedia link for a given name, including country, state, and party\n",
    "def get_wikipedia_link(row):\n",
    "    name = row['Name']\n",
    "    country = row['Country']\n",
    "    state = row['State']\n",
    "    party = row['Party']\n",
    "    \n",
    "    search_queries = [\n",
    "        f\"{name} {country} politician\",\n",
    "        f\"{name} {state} politician\",\n",
    "        f\"{name} {party} politician\"\n",
    "    ]\n",
    "    \n",
    "    for query in search_queries:\n",
    "        search_url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={query}&format=json\"\n",
    "        response = requests.get(search_url).json()\n",
    "        \n",
    "        try:\n",
    "            # Get the first search result page ID\n",
    "            page_id = response['query']['search'][0]['pageid']\n",
    "            page_url = f\"https://en.wikipedia.org/?curid={page_id}\"\n",
    "            \n",
    "            # Get the content of the page\n",
    "            page_content_url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&exintro&explaintext&format=json&pageids={page_id}\"\n",
    "            page_content = requests.get(page_content_url).json()\n",
    "            extract = page_content['query']['pages'][str(page_id)]['extract']\n",
    "            \n",
    "            # Check if the page mentions the person as a politician\n",
    "            if 'politician' in extract.lower():\n",
    "                return page_url\n",
    "        except (IndexError, KeyError):\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Load and process the Control dataset\n",
    "control_file_path = 'C:/Users/Guill/OneDrive/Documents/Control.csv'  # Replace with the path to your actual Control dataset\n",
    "control_df = pd.read_csv(control_file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Remove leading and trailing spaces from the 'Name' column\n",
    "control_df['Name'] = control_df['Name'].str.strip()\n",
    "\n",
    "# Filter out rows that contain 'Ã' in any column\n",
    "control_df = control_df[~control_df.apply(lambda row: row.astype(str).str.contains('Ã').any(), axis=1)]\n",
    "\n",
    "# Clean names in the Control dataset\n",
    "control_df['Name'] = control_df['Name'].apply(clean_name)\n",
    "\n",
    "# Apply the Wikipedia link function to the Control dataset\n",
    "control_df['Wikipedia_Link'] = control_df.apply(get_wikipedia_link, axis=1)\n",
    "\n",
    "# Save the updated Control DataFrame to a new CSV file\n",
    "control_output_file_path = 'C:/Users/Guill/OneDrive/Documents/Control_with_links.csv'\n",
    "control_df.to_csv(control_output_file_path, index=False)\n",
    "\n",
    "# Load and process the Treatment dataset\n",
    "treatment_file_path = 'C:/Users/Guill/OneDrive/Documents/Treatment.csv'  # Replace with the path to your actual Treatment dataset\n",
    "treatment_df = pd.read_csv(treatment_file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Remove leading and trailing spaces from the 'Name' column\n",
    "treatment_df['Name'] = treatment_df['Name'].str.strip()\n",
    "\n",
    "# Filter out rows that contain 'Ã' in any column\n",
    "treatment_df = treatment_df[~treatment_df.apply(lambda row: row.astype(str).str.contains('Ã').any(), axis=1)]\n",
    "\n",
    "# Clean names in the Treatment dataset\n",
    "treatment_df['Name'] = treatment_df['Name'].apply(clean_name)\n",
    "\n",
    "# Apply the Wikipedia link function to the Treatment dataset\n",
    "treatment_df['Wikipedia_Link'] = treatment_df.apply(get_wikipedia_link, axis=1)\n",
    "\n",
    "# Save the updated Treatment DataFrame to a new CSV file\n",
    "treatment_output_file_path = 'C:/Users/Guill/OneDrive/Documents/Treatment_with_links.csv'\n",
    "treatment_df.to_csv(treatment_output_file_path, index=False)\n",
    "\n",
    "# Display the updated DataFrames\n",
    "print(control_df)\n",
    "print(treatment_df)\n",
    "\n",
    "\n",
    "# Step 1: Delete all observations without a link in both datasets\n",
    "control_df = control_df.dropna(subset=['Wikipedia_Link'])\n",
    "treatment_df = treatment_df.dropna(subset=['Wikipedia_Link'])\n",
    "\n",
    "# Step 2: Remove whole rows with duplicate Wikipedia links in Control dataset\n",
    "control_df = control_df.drop_duplicates(subset=['Wikipedia_Link'])\n",
    "\n",
    "# Step 3: Ensure columns are the same\n",
    "# Remove 'Date in' and 'Date Out' from Control, add 'Date of party switch' as NA, and set 'Party' and 'New Party' to the same value\n",
    "control_df = control_df.drop(columns=['Date in', 'Date Out'])\n",
    "control_df['Date of party switch'] = pd.NA\n",
    "control_df['New Party'] = control_df['Party']\n",
    "\n",
    "# Reorder columns to match Treatment dataset\n",
    "control_df = control_df[['Country', 'Name', 'State', 'Date of party switch', 'Party', 'New Party', 'Wikipedia_Link']]\n",
    "treatment_df = treatment_df[['Country', 'Name', 'State', 'Date of party switch', 'Party', 'New Party', 'Wikipedia_Link']]\n",
    "\n",
    "# Step 4: Remove Control observations that are also in Treatment\n",
    "treatment_links = treatment_df['Wikipedia_Link'].tolist()\n",
    "control_df = control_df[~control_df['Wikipedia_Link'].isin(treatment_links)]\n",
    "control_df.to_csv('C:/Users/Guill/OneDrive/Documents/Control1.csv', index=False)\n",
    "treatment_df.to_csv('C:/Users/Guill/OneDrive/Documents/Treatment1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = 'C:/Users/Guill/Downloads/FullDF.csv'\n",
    "\n",
    "# Load the CSV file with utf-8 encoding\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Ensure all Wikipedia_Link entries are strings and handle NaN values\n",
    "df['Wikipedia_Link'] = df['Wikipedia_Link'].astype(str).fillna('')\n",
    "\n",
    "# Define the conversion function with retry mechanism\n",
    "def get_standard_wikipedia_url(curid_url):\n",
    "    # Extract the curid from the URL\n",
    "    curid = curid_url.split('curid=')[-1]\n",
    "    \n",
    "    # Query the Wikipedia API to get the page title with retries\n",
    "    api_url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&pageids={curid}\"\n",
    "    for attempt in range(5):  # Retry up to 5 times\n",
    "        try:\n",
    "            response = requests.get(api_url)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            data = response.json()\n",
    "            # Extract the page title\n",
    "            page = data['query']['pages'][curid]\n",
    "            title = page['title']\n",
    "            # Construct the standard URL and properly encode it\n",
    "            standard_url = f\"https://en.wikipedia.org/wiki/{urllib.parse.quote(title.replace(' ', '_'))}\"\n",
    "            # Decode the URL back to human-readable format\n",
    "            human_readable_url = urllib.parse.unquote(standard_url)\n",
    "            return human_readable_url\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(2)  # Wait for 2 seconds before retrying\n",
    "    print(\"Failed to retrieve the page title after 5 attempts.\")\n",
    "    return curid_url  # Return the original URL if all attempts fail\n",
    "\n",
    "# Apply the conversion function to all Wikipedia_Link entries\n",
    "df['Wikipedia_Link'] = df['Wikipedia_Link'].apply(lambda url: get_standard_wikipedia_url(url) if 'curid=' in url else url)\n",
    "\n",
    "# Save the updated DataFrame with utf-8 encoding\n",
    "output_path = 'C:/Users/Guill/Downloads/FullDF_Updated.csv'\n",
    "df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "print(df.head())\n",
    "\n",
    "print(df.head(23))\n",
    "# TO GET SENTEMENT AND ALL THAT IS ON A DIFFERNT FILE START HERE AGAIN TO CLEAN MORE AND MERGE \n",
    "\n",
    "# Function to preprocess and standardize date formats\n",
    "def preprocess_date(date_str):\n",
    "    if pd.isnull(date_str) or date_str.strip() == '':\n",
    "        return date_str  # Leave it as is if it's empty or NaN\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    if re.match(r'^\\d{4}$', date_str):  # Year only, e.g., '2005'\n",
    "        return f'{date_str}-01-01'\n",
    "    elif re.match(r'^\\d{2}-\\d{2}$', date_str):  # YY-MM, e.g., '18-07'\n",
    "        parts = date_str.split('-')\n",
    "        year, month = '20' + parts[0], parts[1]  # Assuming all dates are post-2005\n",
    "        return f'{year}-{month}-01'\n",
    "    elif re.match(r'^\\d{2}-\\w{3}$', date_str):  # YY-MMM, e.g., '19-Jul'\n",
    "        parts = date_str.split('-')\n",
    "        year, month = '20' + parts[0], parts[1]  # Assuming all dates are post-2005\n",
    "        return f'{year}-{month}-01'\n",
    "    \n",
    "    return date_str  # Leave it as is if it doesn't match the format\n",
    "\n",
    "# Load the data from CSV file\n",
    "full_df = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\FullDF.csv')\n",
    "\n",
    "# Apply preprocessing to the 'Date of party switch' column\n",
    "full_df['Date of party switch'] = full_df['Date of party switch'].apply(preprocess_date)\n",
    "\n",
    "# Ensure dates are in correct datetime format, leave non-matching as is\n",
    "full_df['Date of party switch'] = pd.to_datetime(full_df['Date of party switch'], errors='coerce')\n",
    "\n",
    "# Find the oldest and newest dates, ignoring NaT values\n",
    "oldest_date = full_df['Date of party switch'].min()\n",
    "newest_date = full_df['Date of party switch'].max()\n",
    "\n",
    "# Print the oldest and newest dates\n",
    "print(\"Oldest date:\", oldest_date)\n",
    "print(\"Newest date:\", newest_date)\n",
    "\n",
    "# Save the dataframe to a new CSV file\n",
    "full_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\dateflter1.csv', index=False)\n",
    "\n",
    "# Check if all dates with values are formatted the same\n",
    "def check_date_format(date_series):\n",
    "    # Define the expected format\n",
    "    expected_format = '%Y-%m-%d'\n",
    "    \n",
    "    # Drop NaN values\n",
    "    date_series = date_series.dropna()\n",
    "    \n",
    "    # Convert to datetime and back to string in the expected format\n",
    "    formatted_dates = pd.to_datetime(date_series, errors='coerce').dt.strftime(expected_format)\n",
    "    \n",
    "    # Check if all non-NaN dates match the expected format\n",
    "    consistent = (date_series == formatted_dates).all()\n",
    "    \n",
    "    return consistent\n",
    "\n",
    "consistent_format = check_date_format(full_df['Date of party switch'])\n",
    "\n",
    "# Count how many dates are present in the 'Date of party switch' column\n",
    "date_count = full_df['Date of party switch'].notna().sum()\n",
    "\n",
    "print(\"All dates formatted the same:\", consistent_format)\n",
    "print(\"Number of dates in 'Date of party switch' column:\", date_count)\n",
    "# Ensure 'Date of party switch' is in datetime format\n",
    "full_df['Date of party switch'] = pd.to_datetime(full_df['Date of party switch'], errors='coerce')\n",
    "\n",
    "# Separate rows with and without 'Date of party switch'\n",
    "with_date = full_df.dropna(subset=['Date of party switch'])\n",
    "without_date = full_df[full_df['Date of party switch'].isna()]\n",
    "\n",
    "# Sort by 'Name' and 'Date of party switch'\n",
    "with_date = with_date.sort_values(by=['Name', 'Date of party switch'])\n",
    "\n",
    "# Drop duplicates, keeping the first (earliest date)\n",
    "with_date = with_date.drop_duplicates(subset=['Name', 'Country', 'State', 'Wikipedia_Link'], keep='first')\n",
    "\n",
    "# Concatenate the cleaned dataframes back together\n",
    "cleaned_full_df = pd.concat([with_date, without_date])\n",
    "\n",
    "# Count the number of individual names with a 'Date of party switch'\n",
    "unique_names_with_date = cleaned_full_df.dropna(subset=['Date of party switch'])['Name'].nunique()\n",
    "print(\"Number of individual names with 'Date of party switch':\", unique_names_with_date)\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of individual names with a 'Date of party switch'\n",
    "unique_names_with_date = full_df.dropna(subset=['Date of party switch'])['Name'].nunique()\n",
    "print(\"Number of individual names with 'Date of party switch':\", unique_names_with_date)\n",
    "\n",
    "# Ensure 'Date of party switch' is in datetime format\n",
    "full_df['Date of party switch'] = pd.to_datetime(full_df['Date of party switch'], errors='coerce')\n",
    "\n",
    "# Identify observations where both 'Party' and 'New Party' are filled but 'Date of party switch' is not filled\n",
    "missing_date_switch = full_df[(full_df['Party'].notna()) & (full_df['New Party'].notna()) & (full_df['Date of party switch'].isna())]\n",
    "\n",
    "# Display the identified observations\n",
    "print(missing_date_switch.head(20))\n",
    "# Ensure 'Date of party switch' is in datetime format\n",
    "full_df['Date of party switch'] = pd.to_datetime(full_df['Date of party switch'], errors='coerce')\n",
    "\n",
    "# Drop duplicates, prioritizing rows with 'Date of party switch'\n",
    "full_df = full_df.sort_values(by=['Date of party switch'], na_position='last')\n",
    "full_df = full_df.drop_duplicates(subset=['Name', 'Country', 'State', 'Wikipedia_Link'], keep='first')\n",
    "\n",
    "full_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\dateflter3.csv', index=False)\n",
    "\n",
    "score_df = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\score_all.csv')\n",
    "\n",
    "# Merge score_df with full_df based on the 'Name' column\n",
    "merged_df = pd.merge(score_df, full_df[['Name', 'Country', 'State', 'Date of party switch', 'Party', 'New Party']], on='Name', how='left')\n",
    "\n",
    "# Check if there are any names in score_df that do not have a match in full_df\n",
    "unmatched_names = score_df[~score_df['Name'].isin(full_df['Name'])]['Name']\n",
    "\n",
    "if not unmatched_names.empty:\n",
    "    print(\"Names in score_df that do not have a match in full_df:\")\n",
    "    print(unmatched_names)\n",
    "else:\n",
    "    print(\"All names in score_df have a match in full_df.\")\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\merged_score_full.csv', index=False)\n",
    "\n",
    "merged_df=pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\merged_score_full.csv')\n",
    "# Display unique entries in 'Party' and 'New Party' columns\n",
    "unique_parties = merged_df['Party'].dropna().unique()\n",
    "unique_new_parties = merged_df['New Party'].dropna().unique()\n",
    "\n",
    "print(\"Unique entries in 'Party' column:\")\n",
    "print(unique_parties)\n",
    "\n",
    "print(\"\\nUnique entries in 'New Party' column:\")\n",
    "print(unique_new_parties)\n",
    "\n",
    "# Load the datasets\n",
    "merged_score_full_df = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\merged_score_full.csv')\n",
    "party_afil_df = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\Party afil.csv', encoding='latin1')\n",
    "\n",
    "# Remove spaces only on the outside of the words in 'Country' and 'Party' columns\n",
    "merged_score_full_df['Country'] = merged_score_full_df['Country'].str.strip()\n",
    "merged_score_full_df['Party'] = merged_score_full_df['Party'].str.strip()\n",
    "merged_score_full_df['New Party'] = merged_score_full_df['New Party'].str.strip()\n",
    "\n",
    "party_afil_df['Country'] = party_afil_df['Country'].str.strip()\n",
    "party_afil_df['Party'] = party_afil_df['Party'].str.strip()\n",
    "\n",
    "# Fill empty 'New Party' with 'Party'\n",
    "merged_score_full_df['New Party'].fillna(merged_score_full_df['Party'], inplace=True)\n",
    "\n",
    "# Merging based on Party and Country\n",
    "merged_df = merged_score_full_df.merge(\n",
    "    party_afil_df[['Country', 'Party', 'Party_Alignment', 'Party Right Wing']],\n",
    "    how='left',\n",
    "    left_on=['Country', 'Party'],\n",
    "    right_on=['Country', 'Party']\n",
    ")\n",
    "\n",
    "# Merging based on New Party and Country\n",
    "merged_df = merged_df.merge(\n",
    "    party_afil_df[['Country', 'Party', 'New Party_Alignment', 'New Party Right Wing']],\n",
    "    how='left',\n",
    "    left_on=['Country', 'New Party'],\n",
    "    right_on=['Country', 'Party'],\n",
    "    suffixes=('', '_New')\n",
    ")\n",
    "\n",
    "# Drop redundant columns\n",
    "merged_df = merged_df.drop(columns=['Party_New'])\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\merged_score_full_with_party_info1.csv', index=False)\n",
    "\n",
    "print(merged_df.head())\n",
    "\n",
    "# Load the datasets\n",
    "merged_score_full_with_party_info_df = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\merged_score_full_with_party_info1.csv', encoding='latin1')\n",
    "full_word_analyzed_score_df = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\full word analyzed_score.csv', encoding='latin1')\n",
    "\n",
    "# Merging based on URL and Date\n",
    "final_merged_df = merged_score_full_with_party_info_df.merge(\n",
    "    full_word_analyzed_score_df[['url', 'date', 'word_count']],\n",
    "    how='left',\n",
    "    left_on=['url', 'date'],\n",
    "    right_on=['url', 'date']\n",
    ")\n",
    "\n",
    "\n",
    "# Number of observations before deletion\n",
    "initial_count = len(final_merged_df)\n",
    "\n",
    "# Remove observations missing all three of 'positive', 'neutral', and 'negative'\n",
    "final_merged_df = final_merged_df.dropna(subset=['positive', 'neutral', 'negative'], how='all')\n",
    "\n",
    "# Number of observations after deletion\n",
    "final_count = len(final_merged_df)\n",
    "\n",
    "# Calculate the number and percentage of deleted observations\n",
    "deleted_count = initial_count - final_count\n",
    "deleted_percentage = (deleted_count / initial_count) * 100\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_merged_with_word_count.csv', index=False)\n",
    "\n",
    "# Display the merged dataframe and deletion stats\n",
    "print(f\"Merge complete. Output saved to 'final_merged_with_word_count.csv'.\")\n",
    "print(f\"Number of observations deleted: {deleted_count}\")\n",
    "print(f\"Percentage of observations deleted: {deleted_percentage:.2f}%\")\n",
    "#removing time observation \n",
    "final_merged_df['date'] = pd.to_datetime(final_merged_df['date']).dt.date\n",
    "\n",
    "# Save the final merged dataframe to a new CSV file\n",
    "final_merged_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_DF.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create 'more_left' column\n",
    "final_merged_df['more_left'] = np.where(\n",
    "    ((final_merged_df['Party Right Wing'] == 1) & (final_merged_df['New Party Right Wing'] == 0)) |\n",
    "    (final_merged_df['Party_Alignment'] > final_merged_df['New Party_Alignment']) |\n",
    "    (final_merged_df['Party Right Wing'].isna() & (final_merged_df['New Party Right Wing'] == 0)),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# Create 'more_right' column\n",
    "final_merged_df['more_right'] = np.where(\n",
    "    ((final_merged_df['Party Right Wing'] == 0) & (final_merged_df['New Party Right Wing'] == 1)) |\n",
    "    (final_merged_df['Party_Alignment'] < final_merged_df['New Party_Alignment']) |\n",
    "    (final_merged_df['Party_Alignment'].notna() & final_merged_df['New Party_Alignment'].isna()) |\n",
    "    (final_merged_df['Party Right Wing'].isna() & (final_merged_df['New Party Right Wing'] == 1)),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# Create 'equal' column\n",
    "final_merged_df['equal'] = np.where(\n",
    "    (final_merged_df['Party Right Wing'] == final_merged_df['New Party Right Wing']) &\n",
    "    (final_merged_df['Party_Alignment'] == final_merged_df['New Party_Alignment'])|\n",
    "    (final_merged_df['Party Right Wing'].isna() & (final_merged_df['New Party Right Wing'].isna())),\n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# Ensure date columns are in datetime format\n",
    "final_merged_df['Date of party switch'] = pd.to_datetime(final_merged_df['Date of party switch'], errors='coerce')\n",
    "final_merged_df['date'] = pd.to_datetime(final_merged_df['date'], errors='coerce')\n",
    "\n",
    "# Create 'treated' column\n",
    "final_merged_df['treated'] = np.where(\n",
    "    final_merged_df['Date of party switch'].notna() & (final_merged_df['date'] > final_merged_df['Date of party switch']),\n",
    "    1, 0\n",
    ")\n",
    "#analysis below \n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\FinalDFForAnalysisCleanScore.csv'\n",
    "final_merged_df = pd.read_csv(file_path)\n",
    "final_merged_df = final_merged_df.drop_duplicates()\n",
    "# Split into two DataFrames\n",
    "df_equal = final_merged_df[final_merged_df['equal'] == 1]\n",
    "\n",
    "df_more_left = final_merged_df[(final_merged_df['more_left'] == 1) & (final_merged_df['more_right'] != 1)]\n",
    "df_more_left = pd.concat([df_more_left, df_equal])\n",
    "\n",
    "df_more_right = final_merged_df[(final_merged_df['more_right'] == 1) & (final_merged_df['more_left'] != 1)]\n",
    "df_more_right = pd.concat([df_more_right, df_equal])\n",
    "# Save the split DataFrames to new CSV files\n",
    "df_more_left.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_DF_more_left.csv', index=False)\n",
    "df_more_right.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_DF_more_right.csv', index=False)\n",
    "final_merged_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\FinalDFForAnalysisWithALL.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files into DataFrames\n",
    "df_more_left = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_DF_more_left.csv')\n",
    "df_more_right = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_DF_more_right.csv')\n",
    "final_merged_df = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\FinalDFForAnalysisWithALL.csv')\n",
    "\n",
    "# Load your DataFrame\n",
    "df = final_merged_df\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Group by date and calculate the mean sentiment score\n",
    "avg_sentiment_over_time = df.groupby('date')['score'].mean().reset_index()\n",
    "\n",
    "# Smooth the data using a rolling window of 150 days\n",
    "avg_sentiment_over_time['score'] = avg_sentiment_over_time['score'].rolling(window=150).mean()\n",
    "\n",
    "# Create a plot with a colorblind-friendly color\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(avg_sentiment_over_time['date'], avg_sentiment_over_time['score'], label='Average Sentiment (150-day Rolling Average)',\n",
    "         color=to_rgba('#117733'), linewidth=2)  # Colorblind-friendly green\n",
    "\n",
    "# Customize the plot with labels, title, and grid\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.title('Average Sentiment Over Time for All Politicians', fontsize=16)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)  # Lighter grid lines for better readability\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Create a new column to indicate treated group based on \"Date of party switch\"\n",
    "df['treated_group'] = df['Date_of_party_switch'].notna().astype(int)\n",
    "\n",
    "# Group by date and treated group, then calculate the mean sentiment score\n",
    "avg_sentiment_treated = df.groupby(['date', 'treated_group'])['score'].mean().reset_index()\n",
    "\n",
    "# Smooth the data using a rolling window of 150 days\n",
    "avg_sentiment_treated['score'] = avg_sentiment_treated.groupby('treated_group')['score'].transform(lambda x: x.rolling(window=150).mean())\n",
    "\n",
    "# Prepare a colorblind-friendly color palette\n",
    "colors = ['#117733', '#CC6677']  # Green and pink, colorblind-friendly\n",
    "\n",
    "# Plot the average sentiment over time for treated and untreated groups\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx, treated in enumerate(avg_sentiment_treated['treated_group'].unique()):\n",
    "    group_data = avg_sentiment_treated[avg_sentiment_treated['treated_group'] == treated]\n",
    "    plt.plot(group_data['date'], group_data['score'], label=f'Treated Group: {treated}', color=colors[idx], linewidth=2)\n",
    "\n",
    "# Customize the plot with labels, title, and grid\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.title('Average Sentiment Over Time by Treated Group (Date of Party Switch)', fontsize=16)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)  # Lighter grid lines for better readability\n",
    "plt.show()\n",
    "# Filter for treated=0 and group by date and Party_Alignment\n",
    "avg_sentiment_party_alignment = df[df['treated_group'] == 0].groupby(['date', 'Party_Alignment'])['score'].mean().reset_index()\n",
    "\n",
    "# Smooth the data using a rolling window of 150 days\n",
    "avg_sentiment_party_alignment['score'] = avg_sentiment_party_alignment.groupby('Party_Alignment')['score'].transform(lambda x: x.rolling(window=150).mean())\n",
    "\n",
    "# Prepare a colorblind-friendly color palette\n",
    "colors = ['#88CCEE', '#DDCC77', '#117733', '#CC6677']  # Blue, yellow, green, pink - all colorblind-friendly\n",
    "\n",
    "# Plot the average sentiment over time for treated=0, split by Party_Alignment\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx, alignment in enumerate(avg_sentiment_party_alignment['Party_Alignment'].unique()):\n",
    "    alignment_data = avg_sentiment_party_alignment[avg_sentiment_party_alignment['Party_Alignment'] == alignment]\n",
    "    plt.plot(alignment_data['date'], alignment_data['score'], label=f'Party_Alignment: {alignment}', color=colors[idx % len(colors)], linewidth=2)\n",
    "\n",
    "# Customize the plot with labels, title, and grid\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.title('Average Sentiment Over Time for Treated=0, Split by Party_Alignment (150-day Rolling Average)', fontsize=16)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)  # Lighter and dashed grid lines for better readability\n",
    "plt.show()\n",
    "\n",
    "# Filter for treated=0 and group by date and Party Right Wing\n",
    "avg_sentiment_right_wing = df[df['treated_group'] == 0].groupby(['date', 'Party_Right_Wing'])['score'].mean().reset_index()\n",
    "\n",
    "# Smooth the data using a rolling window of 150 days\n",
    "avg_sentiment_right_wing['score'] = avg_sentiment_right_wing.groupby('Party_Right_Wing')['score'].transform(lambda x: x.rolling(window=150).mean())\n",
    "\n",
    "# Prepare a colorblind-friendly color palette\n",
    "colors = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7']  # A set of distinct colors for colorblind viewers\n",
    "\n",
    "# Plot the average sentiment over time for treated=0, split by Party Right Wing\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx, right_wing in enumerate(avg_sentiment_right_wing['Party_Right_Wing'].unique()):\n",
    "    right_wing_data = avg_sentiment_right_wing[avg_sentiment_right_wing['Party_Right_Wing'] == right_wing]\n",
    "    plt.plot(right_wing_data['date'], right_wing_data['score'], label=f'Party Right Wing: {right_wing}', color=colors[idx % len(colors)], linewidth=2)\n",
    "\n",
    "# Customize the plot with labels, title, and grid\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.title('Average Sentiment Over Time for Treated=0, Split by Party Right Wing (150-day Rolling Average)', fontsize=16)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)  # Lighter and dashed grid lines for better readability\n",
    "plt.show()\n",
    "# Filter for treated=0 and group by Party_Alignment\n",
    "avg_sentiment_party_alignment_bar = df[df['treated_group'] == 0].groupby('Party_Alignment')['score'].mean().reset_index()\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "colors = ['#0072B2', '#E69F00', '#F0E442', '#009E73', '#56B4E9', '#D55E00', '#CC79A7']\n",
    "\n",
    "# Plot bar graph for average sentiment for treated=0, split by Party_Alignment\n",
    "plt.figure(figsize=(15, 10))\n",
    "bars = plt.bar(avg_sentiment_party_alignment_bar['Party_Alignment'], avg_sentiment_party_alignment_bar['score'],\n",
    "               color=colors[:len(avg_sentiment_party_alignment_bar)], width=0.6)  # Adjust bar width here\n",
    "\n",
    "# Customize plot with labels, title\n",
    "plt.xlabel('Party_Alignment', fontsize=14)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.title('Average Sentiment for Treated=0, Split by Party_Alignment', fontsize=16)\n",
    "\n",
    "# Improve aesthetics for the bar chart\n",
    "plt.xticks(rotation=45)  # Rotate labels to fit better if needed\n",
    "plt.gca().set_facecolor('#f0f0f0')  # Light gray background for better contrast\n",
    "plt.grid(True, linestyle='--', alpha=0.5, axis='y')  # Grid only horizontally\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Filter for treated=1 and group by Party_Alignment\n",
    "avg_sentiment_party_alignment_bar = df[df['treated_group'] == 1].groupby('Party_Alignment')['score'].mean().reset_index()\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "colors = ['#0072B2', '#E69F00', '#F0E442', '#009E73', '#56B4E9', '#D55E00', '#CC79A7']\n",
    "\n",
    "# Plot bar graph for average sentiment for treated=0, split by Party_Alignment\n",
    "plt.figure(figsize=(15, 10))\n",
    "bars = plt.bar(avg_sentiment_party_alignment_bar['Party_Alignment'], avg_sentiment_party_alignment_bar['score'],\n",
    "               color=colors[:len(avg_sentiment_party_alignment_bar)], width=0.6)  # Adjust bar width here\n",
    "\n",
    "# Customize plot with labels, title\n",
    "plt.xlabel('Party_Alignment', fontsize=14)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.title('Average Sentiment for Treated=1, Split by Party_Alignment', fontsize=16)\n",
    "\n",
    "# Improve aesthetics for the bar chart\n",
    "plt.xticks(rotation=45)  # Rotate labels to fit better if needed\n",
    "plt.gca().set_facecolor('#f0f0f0')  # Light gray background for better contrast\n",
    "plt.grid(True, linestyle='--', alpha=0.5, axis='y')  # Grid only horizontally\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Filter for treated=1 and group by Party_Alignment\n",
    "avg_sentiment_party_alignment_bar = df.groupby('Party_Alignment')['score'].mean().reset_index()\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "colors = ['#0072B2', '#E69F00', '#F0E442', '#009E73', '#56B4E9', '#D55E00', '#CC79A7']\n",
    "\n",
    "# Plot bar graph for average sentiment for treated=1, split by Party_Alignment\n",
    "plt.figure(figsize=(15, 10))\n",
    "bars = plt.bar(avg_sentiment_party_alignment_bar['Party_Alignment'], avg_sentiment_party_alignment_bar['score'],\n",
    "               color=colors[:len(avg_sentiment_party_alignment_bar)], width=0.6)  # Adjust bar width here\n",
    "\n",
    "# Customize plot with labels, title\n",
    "plt.xlabel('Party_Alignment', fontsize=14)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.title('Average Sentiment, Split by Party_Alignment', fontsize=16)\n",
    "\n",
    "# Improve aesthetics for the bar chart\n",
    "plt.xticks(rotation=45)  # Rotate labels to fit better if needed\n",
    "plt.gca().set_facecolor('#f0f0f0')  # Light gray background for better contrast\n",
    "plt.grid(True, linestyle='--', alpha=0.5, axis='y')  # Grid only horizontally\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "# Colorblind-friendly color palette\n",
    "colors = ['#0072B2', '#009E73', '#56B4E9', '#D55E00']  # Dark blue, teal, light blue, orange\n",
    "\n",
    "# Plot function for average sentiment, split by Party Right Wing\n",
    "def plot_avg_sentiment(df, treated, title):\n",
    "    avg_sentiment_right_wing_bar = df[df['treated_group'] == treated].groupby('Party_Right_Wing')['score'].mean().reset_index()\n",
    "    plt.figure(figsize=(15, 7))  # Reduced height for a slimmer look\n",
    "    bars = plt.bar(avg_sentiment_right_wing_bar['Party_Right_Wing'], avg_sentiment_right_wing_bar['score'],\n",
    "                   color=colors[:len(avg_sentiment_right_wing_bar)], width=0.5, edgecolor='black')  # Less width and edge color for definition\n",
    "    plt.xlabel('Party Right Wing', fontsize=12)\n",
    "    plt.ylabel('Average Sentiment Score', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=10)  # Smaller font size for axis ticks\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.gca().set_facecolor('#f0f0f0')  # Light gray background for better contrast\n",
    "    plt.grid(True, linestyle='--', alpha=0.5, axis='y', which='major')  # Grid only horizontally\n",
    "    plt.tight_layout()  # Adjust layout to make sure everything fits without overlapping\n",
    "    plt.show()\n",
    "\n",
    "# Call function for treated=0\n",
    "plot_avg_sentiment(df, 0, 'Average Sentiment for Treated=0, Split by Party Right Wing')\n",
    "\n",
    "# Call function for treated=1\n",
    "plot_avg_sentiment(df, 1, 'Average Sentiment for Treated=1, Split by Party Right Wing')\n",
    "\n",
    "# Filter for treated=1 and group by Party Right Wing\n",
    "avg_sentiment_right_wing_bar = df.groupby('Party_Right_Wing')['score'].mean().reset_index()\n",
    "\n",
    "## Colorblind-friendly color palette, subtle and professional\n",
    "colors = ['#0072B2', '#56B4E9', '#009E73', '#E69F00']  # Blue, light blue, teal, orange\n",
    "\n",
    "# Plot bar graph for average sentiment for treated=1, split by Party Right Wing\n",
    "plt.figure(figsize=(15, 7))  # Adjusted for a slimmer look\n",
    "bars = plt.bar(avg_sentiment_right_wing_bar['Party_Right_Wing'], avg_sentiment_right_wing_bar['score'],\n",
    "               color=colors[:len(avg_sentiment_right_wing_bar)], width=0.5, edgecolor='black')  # More elegant bar width and definition\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Party Right Wing', fontsize=12)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=12)\n",
    "plt.title('Average Sentiment, Split by Party Right Wing', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=10)  # Smaller font size for labels\n",
    "plt.yticks(fontsize=10)\n",
    "plt.gca().set_facecolor('#f0f0f0')  # Subtle background color for better contrast\n",
    "plt.grid(True, linestyle='--', alpha=0.5, axis='y')  # Subtle grid for better readability\n",
    "plt.tight_layout()  # Ensures everything fits well without overlapping\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Colorblind-friendly color palette\n",
    "colors = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7']  # Orange, sky blue, teal, yellow, dark blue, red-orange, pink\n",
    "\n",
    "def plot_overlap_line_graph(df, group_col, title):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    color_index = 0  # Index to keep track of colors for lines\n",
    "    for treated in df['treated_group'].unique():\n",
    "        for group in df[group_col].unique():\n",
    "            group_data = df[(df['treated_group'] == treated) & (df[group_col] == group)].groupby('date')['score'].mean().reset_index()\n",
    "            group_data['score'] = group_data['score'].rolling(window=150).mean()  # Smooth the data\n",
    "            # Assign a color from the palette and increment the index\n",
    "            plt.plot(group_data['date'], group_data['score'], label=f'Treated={treated}, {group_col}={group}', color=colors[color_index % len(colors)])\n",
    "            color_index += 1\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Sentiment Score')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best', fontsize='small', title_fontsize='medium')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example call to the function\n",
    "plot_overlap_line_graph(df, 'Party_Alignment', 'Average Sentiment Over Time Overlapping Treated Groups, Split by Party_Alignment (150-day Rolling Average)')\n",
    "plot_overlap_line_graph(df, 'Party Right Wing', 'Average Sentiment Over Time Overlapping Treated Groups, Split by Party Right Wing (7-day Rolling Average)')\n",
    "\n",
    "date_ranges = [(2000, 2006), (2007, 2011), (2012, 2019), (2020, 2024)]\n",
    "\n",
    "def plot_bar_graph(df, group_col, title, xlabel='Group', ylabel='Average Sentiment Score'):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    group_data = df.groupby(group_col)['score'].mean().reset_index()\n",
    "    plt.bar(group_data[group_col], group_data['score'])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_bar_graph_by_date_range(df, group_col, date_ranges, treated_label):\n",
    "    for start, end in date_ranges:\n",
    "        date_range_df = df[(df['date'].dt.year >= start) & (df['date'].dt.year <= end)]\n",
    "        plot_bar_graph(date_range_df, group_col, f'Average Sentiment ({start}-{end}) by {group_col} for Treated={treated_label}')\n",
    "\n",
    "plot_bar_graph_by_date_range(df[df['treated'] == 0], 'Party_Alignment', date_ranges, treated_label=0)\n",
    "plot_bar_graph_by_date_range(df[df['treated'] == 1], 'Party_Alignment', date_ranges, treated_label=1)\n",
    "plot_bar_graph_by_date_range(df[df['treated'] == 0], 'Party Right Wing', date_ranges, treated_label=0)\n",
    "plot_bar_graph_by_date_range(df[df['treated'] == 1], 'Party Right Wing', date_ranges, treated_label=1)\n",
    "\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "colors = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7']  # Orange, sky blue, teal, yellow, dark blue, red-orange, pink\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "countries = df['Country'].unique()\n",
    "color_index = 0  # Index to manage color assignment for each country\n",
    "\n",
    "for country in countries:\n",
    "    country_data = df[df['Country'] == country]\n",
    "    avg_sentiment_over_time = country_data.groupby('date')['score'].mean().reset_index()\n",
    "    avg_sentiment_over_time['score'] = avg_sentiment_over_time['score'].rolling(window=150).mean()  # Smooth the data\n",
    "    plt.plot(avg_sentiment_over_time['date'], avg_sentiment_over_time['score'], label=f'Average Sentiment in {country}', color=colors[color_index % len(colors)])\n",
    "    color_index += 1\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=12)\n",
    "plt.title('Average Sentiment Over Time for All Politicians by Country', fontsize=14)\n",
    "plt.legend(loc='best', fontsize='small', title_fontsize='medium')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 10))\n",
    "for country in countries:\n",
    "    country_data = df[df['Country'] == country]\n",
    "    country_data['treated_group'] = country_data['Date of party switch'].notna().astype(int)\n",
    "    avg_sentiment_treated = country_data.groupby(['date', 'treated_group'])['score'].mean().reset_index()\n",
    "    avg_sentiment_treated['score'] = avg_sentiment_treated.groupby('treated_group')['score'].transform(lambda x: x.rolling(window=150).mean())\n",
    "    for treated in avg_sentiment_treated['treated_group'].unique():\n",
    "        group_data = avg_sentiment_treated[avg_sentiment_treated['treated_group'] == treated]\n",
    "        plt.plot(group_data['date'], group_data['score'], label=f'{country}, Treated Group: {treated}')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.title('Average Sentiment Over Time by Treated Group and Country')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "colors = ['#0072B2', '#E69F00', '#F0E442', '#009E73', '#56B4E9', '#D55E00', '#CC79A7']  # Blue, orange, yellow, teal, sky blue, red-orange, pink\n",
    "\n",
    "countries = df['Country'].unique()\n",
    "\n",
    "for country in countries:\n",
    "    country_data = df[(df['Country'] == country) & (df['treated_group'] == 0)]\n",
    "    avg_sentiment_party_alignment = country_data.groupby(['date', 'Party_Alignment'])['score'].mean().reset_index()\n",
    "    avg_sentiment_party_alignment['score'] = avg_sentiment_party_alignment.groupby('Party_Alignment')['score'].transform(lambda x: x.rolling(window=120).mean())\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    color_index = 0  # Index to manage color assignment for each party alignment\n",
    "    for alignment in avg_sentiment_party_alignment['Party_Alignment'].unique():\n",
    "        alignment_data = avg_sentiment_party_alignment[avg_sentiment_party_alignment['Party_Alignment'] == alignment]\n",
    "        plt.plot(alignment_data['date'], alignment_data['score'], label=f'Party_Alignment: {alignment}', color=colors[color_index % len(colors)])\n",
    "        color_index += 1\n",
    "\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Sentiment Score', fontsize=12)\n",
    "    plt.title(f'Average Sentiment Over Time for Treated=0 in {country}, Split by Party_Alignment', fontsize=14)\n",
    "    plt.legend(loc='best', fontsize='small', title_fontsize='medium')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for country in countries:\n",
    "    country_data = df[(df['Country'] == country) & (df['treated'] == 0)]\n",
    "    avg_sentiment_right_wing = country_data.groupby(['date', 'Party Right Wing'])['score'].mean().reset_index()\n",
    "    avg_sentiment_right_wing['score'] = avg_sentiment_right_wing.groupby('Party Right Wing')['score'].transform(lambda x: x.rolling(window=150).mean())\n",
    "\n",
    "    for right_wing in avg_sentiment_right_wing['Party Right Wing'].unique():\n",
    "        right_wing_data = avg_sentiment_right_wing[avg_sentiment_right_wing['Party Right Wing'] == right_wing]\n",
    "        plt.plot(right_wing_data['date'], right_wing_data['score'], label=f'{country}, Party Right Wing: {right_wing}')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.title('Average Sentiment Over Time for Treated=0, Split by Party Right Wing and Country')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Colorblind-friendly color palette, subtle yet distinct\n",
    "colors = ['#0072B2', '#D55E00', '#009E73', '#56B4E9', '#CC79A7']  # Dark blue, red-orange, teal, light blue, pink\n",
    "\n",
    "countries = df['Country'].unique()\n",
    "\n",
    "for country in countries:\n",
    "    country_data = df[df['Country'] == country]\n",
    "    avg_sentiment_right_wing_bar = country_data.groupby('Party_Right_Wing')['score'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))  # Slightly smaller for a more refined look\n",
    "    bars = plt.bar(avg_sentiment_right_wing_bar['Party_Right_Wing'], avg_sentiment_right_wing_bar['score'],\n",
    "                   color=colors[:len(avg_sentiment_right_wing_bar)], width=0.6, edgecolor='black')  # Add edgecolor for definition\n",
    "    \n",
    "    plt.xlabel('Party Right Wing', fontsize=12)\n",
    "    plt.ylabel('Average Sentiment Score', fontsize=12)\n",
    "    plt.title(f'Average Sentiment by Party Right Wing in {country}', fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=10)  # Rotate labels to fit better if needed\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.gca().set_facecolor('#f0f0f0')  # Light gray background for better contrast\n",
    "    plt.grid(True, linestyle='--', alpha=0.5, axis='y')  # Subtle grid for better readability\n",
    "    plt.tight_layout()  # Adjust layout to make sure everything fits without overlapping\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the average number of words split by country\n",
    "avg_word_count_by_country = df.groupby('Country')['word_count'].mean().reset_index()\n",
    "\n",
    "# Colorblind-friendly color palette, elegant and suitable for all viewers\n",
    "colors = ['#0072B2', '#009E73', '#56B4E9', '#D55E00', '#CC79A7']  # Dark blue, teal, light blue, red-orange, pink\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjusted for a more refined and proportional look\n",
    "bars = plt.bar(avg_word_count_by_country['Country'], avg_word_count_by_country['word_count'],\n",
    "               color=colors[:len(avg_word_count_by_country)], width=0.6, edgecolor='black')  # Uniform colors and defined edges\n",
    "\n",
    "plt.xlabel('Country', fontsize=12)\n",
    "plt.ylabel('Average Number of Words', fontsize=12)\n",
    "plt.title('Average Number of Words by Country', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=10)  # Rotate labels to ensure they are readable\n",
    "plt.yticks(fontsize=10)\n",
    "plt.gca().set_facecolor('#f0f0f0')  # Soft grey background for contrast improvement\n",
    "plt.grid(True, linestyle='--', alpha=0.5, axis='y')  # Refined grid on y-axis only\n",
    "plt.tight_layout()  # Ensure all labels and titles fit well within the canvas\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Prepare data for regression\n",
    "df_clean = df.dropna(subset=['score', 'Party_Alignment'])\n",
    "\n",
    "# Convert categorical 'Party_Alignment' to numeric if not already numeric\n",
    "# This assumes 'Party_Alignment' is categorical and maps each unique category to a unique integer\n",
    "if df_clean['Party_Alignment'].dtype == 'object':\n",
    "    df_clean['Party_Alignment'] = pd.Categorical(df_clean['Party_Alignment']).codes\n",
    "\n",
    "# Linear regression using seaborn with colorblind-friendly settings\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='Party_Alignment', y='score', data=df_clean, scatter_kws={'s': 50, 'alpha': 0.5}, line_kws={'color': '#0072B2'})\n",
    "plt.xlabel('Party Alignment', fontsize=12)\n",
    "plt.ylabel('Sentiment Score', fontsize=12)\n",
    "plt.title('Sentiment vs. Party Alignment', fontsize=14)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)  # Enhance grid visibility for better measurement reading\n",
    "plt.show()\n",
    "\n",
    "# Linear regression using statsmodels for more details\n",
    "X = df_clean['Party_Alignment']\n",
    "y = df_clean['score']\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(model.summary())\n",
    "# Select relevant columns for correlation matrix\n",
    "corr_df = df[['score', 'Party_Right_Wing']]\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "# Plot the correlation matrix using a colorblind-friendly palette\n",
    "plt.figure(figsize=(8, 6))\n",
    "# 'viridis' is a good choice for colorblind-friendly palettes; it's perceptually uniform and looks good in black and white as well.\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='viridis', center=0)\n",
    "plt.title('Correlation Matrix: Sentiment with Party Right Wing', fontsize=12)\n",
    "plt.xticks(fontsize=10)  # Adjust font size for readability\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()\n",
    "# Drop rows with missing values in the relevant columns\n",
    "df_clean = df.dropna(subset=['score', 'Party_Alignment', 'Country'])\n",
    "\n",
    "# Convert the 'Country' column to dummy variables\n",
    "df_dummies = pd.get_dummies(df_clean, columns=['Country'], drop_first=True)\n",
    "\n",
    "# Define the independent variables (including dummy variables for countries)\n",
    "X = df_dummies[['Party_Alignment'] + [col for col in df_dummies.columns if col.startswith('Country_')]]\n",
    "y = df_dummies['score']\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the regression results\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\FinalDFForAnalysisWithALL.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['ever_treated'] = df['Date_of_party_switch'].notna().astype(int)\n",
    "df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\FinalDFForAnalysisWithALL.csv')\n",
    "\n",
    "# Convert date to datetime format for accurate time calculations\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Function to process each individual\n",
    "def process_individual(group):\n",
    "    if group['ever_treated'].iloc[0] == 1:\n",
    "        # Check if there's a treatment time\n",
    "        if not any(group['treated_dummy'] == 1):\n",
    "            return pd.DataFrame()  # Return empty DataFrame if no treatment time\n",
    "        \n",
    "        # Identify treatment time\n",
    "        treatment_time = group[group['treated_dummy'] == 1].index[0]\n",
    "        treatment_date = group.loc[treatment_time, 'date']\n",
    "\n",
    "        # Select up to 4 pre-treatment observations\n",
    "        pre_treatment = group[group['treated_dummy'] == 0].iloc[-4:]\n",
    "\n",
    "        # Select up to 4 post-treatment observations\n",
    "        post_treatment = group[group['treated_dummy'] == 1].iloc[:4]\n",
    "\n",
    "        # Filter out observations beyond 2 years before and after treatment\n",
    "        max_date = treatment_date + pd.DateOffset(years=2)\n",
    "        min_date = treatment_date - pd.DateOffset(years=2)\n",
    "        group = group[(group['date'] <= max_date) & (group['date'] >= min_date)]\n",
    "        \n",
    "        # Combine pre-treatment and post-treatment observations\n",
    "        combined = pd.concat([pre_treatment, post_treatment])\n",
    "        \n",
    "        return combined\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "# Apply processing to each individual based on the 'Name' column\n",
    "processed_df = df.groupby('Name').apply(process_individual).reset_index(drop=True)\n",
    "\n",
    "# Save the processed data to a new CSV file\n",
    "processed_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\Processed_FinalDF.csv', index=False)\n",
    "\n",
    "data = pd.read_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\Processed_FinalDF.csv')\n",
    "# Define the treatment and control groups based on the criteria\n",
    "left_treatment_group = data[(data['more_left'] == 1)]\n",
    "right_treatment_group = data[(data['more_right'] == 1)]\n",
    "control_group = data[data['ever_treated'] == 0]\n",
    "\n",
    "# Combine the left treatment group with the control group\n",
    "left_treatment_plus_control = pd.concat([left_treatment_group, control_group])\n",
    "\n",
    "# Combine the right treatment group with the control group\n",
    "right_treatment_plus_control = pd.concat([right_treatment_group, control_group])\n",
    "\n",
    "# Now you have two DataFrames:\n",
    "# left_treatment_plus_control - contains the left treatment group and control group\n",
    "# right_treatment_plus_control - contains the right treatment group and control group\n",
    "\n",
    "right_treatment_plus_control.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\RightDF.csv', index=False)\n",
    "left_treatment_plus_control.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\LeftDF.csv', index=False)\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Filter out rows with missing word_count\n",
    "\n",
    "# Grouping by Name to calculate per-politician statistics\n",
    "per_politician_stats_corrected = data.groupby(['Name']).agg(\n",
    "    avg_score_per_politician=('score', 'mean'),\n",
    "    avg_word_count_per_politician=('word_count', 'mean'),\n",
    "    Party_Alignment=('Party_Alignment', 'mean'),\n",
    "    ever_treated=('ever_treated', 'first'),\n",
    "    Party_Right_Wing=('Party_Right_Wing', 'first'),\n",
    "    Country=('Country', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Function to calculate mean and standard error\n",
    "def mean_and_se(series):\n",
    "    mean = series.mean()\n",
    "    se = np.std(series, ddof=1) / np.sqrt(series.count())\n",
    "    return mean, se\n",
    "\n",
    "# Calculating summary statistics for Treatment and Control groups\n",
    "treatment_stats_corrected = per_politician_stats_corrected[per_politician_stats_corrected['ever_treated'] == 1.0]\n",
    "control_stats_corrected = per_politician_stats_corrected[per_politician_stats_corrected['ever_treated'] == 0.0]\n",
    "\n",
    "# Right Wing and Left Wing groups\n",
    "right_wing_stats_corrected = per_politician_stats_corrected[per_politician_stats_corrected['Party_Right_Wing'] == 1.0]\n",
    "left_wing_stats_corrected = per_politician_stats_corrected[per_politician_stats_corrected['Party_Right_Wing'] == 0.0]\n",
    "\n",
    "# By Country groups\n",
    "usa_stats_corrected = per_politician_stats_corrected[per_politician_stats_corrected['Country'] == 'USA']\n",
    "canada_stats_corrected = per_politician_stats_corrected[per_politician_stats_corrected['Country'] == 'Canada']\n",
    "uk_stats_corrected = per_politician_stats_corrected[per_politician_stats_corrected['Country'] == 'UK']\n",
    "\n",
    "# Creating the summary table\n",
    "summary_table = {\n",
    "    'Group': ['Treatment', 'Control', 'Right Wing', 'Left Wing', 'USA', 'Canada', 'UK'],\n",
    "    'N': [\n",
    "        treatment_stats_corrected['Name'].nunique(),\n",
    "        control_stats_corrected['Name'].nunique(),\n",
    "        right_wing_stats_corrected['Name'].nunique(),\n",
    "        left_wing_stats_corrected['Name'].nunique(),\n",
    "        usa_stats_corrected['Name'].nunique(),\n",
    "        canada_stats_corrected['Name'].nunique(),\n",
    "        uk_stats_corrected['Name'].nunique()\n",
    "    ],\n",
    "    'Average Score (SE)': [\n",
    "        f\"{mean_and_se(treatment_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(treatment_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(control_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(control_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(right_wing_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(right_wing_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(left_wing_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(left_wing_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(usa_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(usa_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(canada_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(canada_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(uk_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(uk_stats_corrected['avg_score_per_politician'])[1]:.2f})\"\n",
    "    ],\n",
    "    'Word Count (SE)': [\n",
    "        f\"{mean_and_se(treatment_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(treatment_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(control_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(control_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(right_wing_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(right_wing_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(left_wing_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(left_wing_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(usa_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(usa_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(canada_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(canada_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(uk_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(uk_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\"\n",
    "    ],\n",
    "    'Party Alignment (SE)': [\n",
    "        f\"{mean_and_se(treatment_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(treatment_stats_corrected['Party_Alignment'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(control_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(control_stats_corrected['Party_Alignment'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(right_wing_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(right_wing_stats_corrected['Party_Alignment'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(left_wing_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(left_wing_stats_corrected['Party_Alignment'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(usa_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(usa_stats_corrected['Party_Alignment'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(canada_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(canada_stats_corrected['Party_Alignment'])[1]:.2f})\",\n",
    "        f\"{mean_and_se(uk_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(uk_stats_corrected['Party_Alignment'])[1]:.2f})\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Recalculate All Politicians with corrected Party Alignment\n",
    "all_politicians_row_corrected = {\n",
    "    'Group': 'All Politicians',\n",
    "    'N': all_politicians_stats_corrected['Name'].nunique(),\n",
    "    'Average Score (SE)': f\"{mean_and_se(all_politicians_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(all_politicians_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "    'Word Count (SE)': f\"{mean_and_se(all_politicians_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(all_politicians_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "    'Party Alignment (SE)': f\"{mean_and_se(all_politicians_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(all_politicians_stats_corrected['Party_Alignment'])[1]:.2f})\"\n",
    "}\n",
    "\n",
    "# Append the corrected \"All Politicians\" row\n",
    "summary_df_corrected = summary_df_corrected.append(all_politicians_row_corrected, ignore_index=True)\n",
    "\n",
    "# Calculating statistics for the \"Pre Treatment\" group\n",
    "\n",
    "# Filter the dataset for pre-treatment observations\n",
    "pre_treatment_stats_corrected = data[(data['ever_treated'] == 1.0) & (data['treated_dummy'] == 0)]\n",
    "\n",
    "# Grouping by Name to calculate per-politician pre-treatment statistics\n",
    "pre_treatment_per_politician_stats_corrected = pre_treatment_stats_corrected.groupby(['Name']).agg(\n",
    "    avg_score_per_politician=('score', 'mean'),\n",
    "    avg_word_count_per_politician=('word_count', 'mean'),\n",
    "    Party_Alignment=('Party_Alignment', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Pre Treatment row calculation\n",
    "pre_treatment_row_corrected = {\n",
    "    'Group': 'Pre Treatment',\n",
    "    'N': pre_treatment_per_politician_stats_corrected['Name'].nunique(),\n",
    "    'Average Score (SE)': f\"{mean_and_se(pre_treatment_per_politician_stats_corrected['avg_score_per_politician'])[0]:.2f} ({mean_and_se(pre_treatment_per_politician_stats_corrected['avg_score_per_politician'])[1]:.2f})\",\n",
    "    'Word Count (SE)': f\"{mean_and_se(pre_treatment_per_politician_stats_corrected['avg_word_count_per_politician'])[0]:.2f} ({mean_and_se(pre_treatment_per_politician_stats_corrected['avg_word_count_per_politician'])[1]:.2f})\",\n",
    "    'Party Alignment (SE)': f\"{mean_and_se(pre_treatment_per_politician_stats_corrected['Party_Alignment'])[0]:.2f} ({mean_and_se(pre_treatment_per_politician_stats_corrected['Party_Alignment'])[1]:.2f})\"\n",
    "}\n",
    "\n",
    "# Remove any previous \"Pre Treatment\" row if present\n",
    "summary_df_corrected = summary_df_corrected[summary_df_corrected['Group'] != 'Pre Treatment']\n",
    "\n",
    "# Append the corrected \"Pre Treatment\" row\n",
    "summary_df_corrected = summary_df_corrected.append(pre_treatment_row_corrected, ignore_index=True)\n",
    "\n",
    "# Display the final corrected summary table with all groups\n",
    "print(summary_df_corrected)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to convert the summary DataFrame to a LaTeX table\n",
    "def dataframe_to_latex(df: DataFrame, filename: str = None):\n",
    "    latex_table = df.to_latex(index=False, float_format=\"%.2f\", column_format=\"|l|c|c|c|c|\", header=True, escape=False)\n",
    "    if filename:\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(latex_table)\n",
    "    return latex_table\n",
    "\n",
    "# Generating the LaTeX table and optionally saving it to a file\n",
    "latex_table = dataframe_to_latex(summary_df_corrected, 'summary_statistics.tex')\n",
    "\n",
    "# Print the LaTeX table\n",
    "print(latex_table)\n",
    "# Save the split DataFrames to new CSV files\n",
    "df_more_left.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_DF_more_left.csv', index=False)\n",
    "df_more_right.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\final_DF_more_right.csv', index=False)\n",
    "final_merged_df.to_csv('C:\\\\Users\\\\Guill\\\\OneDrive\\\\Documents\\\\School\\\\Masters\\\\Sem 3\\\\FinalDFForAnalysisWithALL.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
